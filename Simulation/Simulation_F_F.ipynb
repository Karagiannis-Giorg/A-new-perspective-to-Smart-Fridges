{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMa/cKpghZhUNXGH73GnRgs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karagiannis-Giorg/A-new-perspective-to-Smart-Fridges/blob/main/Simulation/Simulation_F_F.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2aJEZbsty6D",
        "outputId": "4d2719f6-0e6c-448f-a419-7ca228678710"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
            "  Cloning https://github.com/facebookresearch/detectron2.git to /tmp/pip-req-build-2fkh6nbw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-2fkh6nbw\n",
            "  Resolved https://github.com/facebookresearch/detectron2.git to commit 9604f5995cc628619f0e4fd913453b4d7d61db3f\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.10.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.5.0)\n",
            "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (3.1.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (2.18.0)\n",
            "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl.metadata (370 bytes)\n",
            "Collecting omegaconf<2.4,>=2.1 (from detectron2==0.6)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting black (from detectron2==0.6)\n",
            "  Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from detectron2==0.6) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (6.0.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (8.1.8)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black->detectron2==0.6) (4.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->detectron2==0.6) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6) (3.0.2)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading black-25.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: detectron2, fvcore, antlr4-python3-runtime\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp311-cp311-linux_x86_64.whl size=6380981 sha256=35670588e5710c68a297e59c02a9448c6b01f656660d5a2d14837c952e1c18a8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jc2gh4nz/wheels/17/d9/40/60db98e485aa9455d653e29d1046601ce96fe23647f60c1c5a\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=26c59daa2bf839aa1a40270e109296cbd7abd3be789001ac119e41b98ea7f8c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=e08e3b38260319d0e62a27759c04858f926cf6de20d0ba38e32262d2a85b4ddb\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built detectron2 fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore, detectron2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-25.1.0 detectron2-0.6 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-3.1.1 yacs-0.1.8\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-1.1.54-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.1.31)\n",
            "Collecting idna==3.7 (from roboflow)\n",
            "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
            "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
            "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv (from roboflow)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.3.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
            "Collecting filetype (from roboflow)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.55.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.1)\n",
            "Downloading roboflow-1.1.54-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, python-dotenv, opencv-python-headless, idna, roboflow\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 python-dotenv-1.0.1 roboflow-1.1.54\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting ngrok\n",
            "  Downloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ngrok-1.4.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, ngrok, pydeck, streamlit\n",
            "Successfully installed ngrok-1.4.0 pydeck-0.9.1 streamlit-1.42.0 watchdog-6.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "!pip install roboflow\n",
        "!pip install streamlit ngrok opencv-python-headless\n",
        "!pip install pyngrok\n",
        "!ngrok config add-authtoken 2rXJVXzr26fTTIW0Qj1torNd6Il_iiVaeFHCJtw5zki9ubVc\n",
        "\n",
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "ROOT_DIR = '/content/gdrive/My Drive/Georgios_Karagiannis/Code'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"jrkIR8tZdg5Y8uIaKmwZ\")\n",
        "project = rf.workspace(\"smart-fridge-jvm9v\").project(\"level1_images\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "rf = Roboflow(api_key=\"jrkIR8tZdg5Y8uIaKmwZ\")\n",
        "project = rf.workspace(\"smart-fridge-jvm9v\").project(\"smart-fridge-p5rw0\")\n",
        "version = project.version(9)\n",
        "dataset = version.download(\"coco\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xctSll2wvjzx",
        "outputId": "70abe542-15c8-485d-847a-865ae2bf5d67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in level1_images-2 to coco:: 100%|██████████| 33118/33118 [00:00<00:00, 47776.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to level1_images-2 in coco:: 100%|██████████| 915/915 [00:00<00:00, 6517.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Smart-Fridge-9 to coco:: 100%|██████████| 34493/34493 [00:00<00:00, 45199.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Smart-Fridge-9 in coco:: 100%|██████████| 1175/1175 [00:00<00:00, 6043.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile smart_fridge_app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.data.catalog import DatasetCatalog\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "import json\n",
        "\n",
        "# Καταχώριση των δεδομένων COCO και προσθήκη των κατηγοριών\n",
        "def register_coco_instances_with_classes(dataset_name, json_file, image_dir):\n",
        "    # Εγγραφή των δεδομένων COCO\n",
        "    register_coco_instances(dataset_name, {}, json_file, image_dir)\n",
        "\n",
        "    # Ανάγνωση του αρχείου json για να πάρουμε τις κατηγορίες\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Λήψη των κατηγοριών από το αρχείο COCO\n",
        "    thing_classes = [category['name'] for category in coco_data['categories']]\n",
        "\n",
        "    # Ενημέρωση του metadata για να περιλαμβάνει τις κατηγορίες\n",
        "    metadata = MetadataCatalog.get(dataset_name)\n",
        "    metadata.thing_classes = thing_classes\n",
        "\n",
        "# Εγγραφή των δεδομένων για train, val, test και ενημέρωση των κατηγοριών\n",
        "register_coco_instances_with_classes(\"my_dataset_train_level1\", \"/content/level1_images-2/train/_annotations.coco.json\", \"/content/level1_images-2/train\")\n",
        "register_coco_instances_with_classes(\"my_dataset_val_level1\", \"/content/level1_images-2/valid/_annotations.coco.json\", \"/content/level1_images-2/valid\")\n",
        "register_coco_instances_with_classes(\"my_dataset_test_level1\", \"/content/level1_images-2/test/_annotations.coco.json\", \"/content/level1_images-2/test\")\n",
        "\n",
        "register_coco_instances_with_classes(\"my_dataset_train_level2\", \"/content/Smart-Fridge-9/train/_annotations.coco.json\", \"/content/Smart-Fridge-9/train\")\n",
        "register_coco_instances_with_classes(\"my_dataset_val_level2\", \"/content/Smart-Fridge-9/valid/_annotations.coco.json\", \"/content/Smart-Fridge-9/valid\")\n",
        "register_coco_instances_with_classes(\"my_dataset_test_level2\", \"/content/Smart-Fridge-9/test/_annotations.coco.json\", \"/content/Smart-Fridge-9/test\")\n",
        "\n",
        "\n",
        "def setup_faster_rcnn_level1():\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(\"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/config1.yml\")\n",
        "    cfg.MODEL.WEIGHTS = \"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level1/model_final.pth\"\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75\n",
        "    predictor1 = DefaultPredictor(cfg)\n",
        "    metadata1 = MetadataCatalog.get(\"my_dataset_train_level1\")\n",
        "    return predictor1, metadata1\n",
        "\n",
        "def setup_faster_rcnn_level2():\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(\"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/config.yml\")\n",
        "    cfg.MODEL.WEIGHTS = \"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level2/model_final.pth\"\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75\n",
        "    predictor2 = DefaultPredictor(cfg)\n",
        "    metadata2 = MetadataCatalog.get(\"my_dataset_train_level2\")\n",
        "    return predictor2, metadata2\n",
        "\n",
        "# Ανάλυση Εικόνας\n",
        "def analyze_image(image, temperature, humidity, predictor1, predictor2, metadata1, metadata2):\n",
        "    im = cv2.imread(image)\n",
        "    outputs1 = predictor1(im)\n",
        "    pred_classes_indices_level1 = outputs1[\"instances\"].pred_classes.cpu().numpy()\n",
        "    pred_classes_names_level1 = [metadata1.thing_classes[i] for i in pred_classes_indices_level1]\n",
        "\n",
        "    if \"Tomato\" in pred_classes_names_level1:\n",
        "        outputs2 = predictor2(im)\n",
        "        pred_classes_indices_level2 = outputs2[\"instances\"].pred_classes.cpu().numpy()\n",
        "        pred_classes_names_level2 = [metadata2.thing_classes[i] for i in pred_classes_indices_level2]\n",
        "        ripeness_message = []\n",
        "\n",
        "        for ripeness in pred_classes_names_level2:\n",
        "            if ripeness == \"over-ripe\":\n",
        "                ripeness_message.append(\"Over-ripe tomatoes detected. Discard them immediatley before they start to effect the other fruits.\")\n",
        "            elif ripeness == \"ripe\":\n",
        "                if 7 <= temperature <= 10 and humidity > 70:\n",
        "                    ripeness_message.append(\"Ripe tomatoes detected. They are edible for up to 4 days. If you want your tomatoes to last longer, you should store them somewhere cooler.\")\n",
        "                elif temperature < 7:\n",
        "                    ripeness_message.append(\"You should put your ripe tomatoes somewhere warmer to avoid chilling injuries.\")\n",
        "                else:\n",
        "                    ripeness_message.append(\"YYour tomatoes will ripen quickly, but you should bear in mind that they are susceptible to pathogens.\")\n",
        "            elif ripeness == \"half-ripe\":\n",
        "                if 8 <= temperature <= 10 and humidity > 60:\n",
        "                    ripeness_message.append(\"Half-ripe tomatoes detected. They are not edible but can be stored for up to 7 days untill they are edible.If you want to ripe them more slower , you should store them somewhere cooler and with less humidity\")\n",
        "                elif temperature < 8:\n",
        "                    ripeness_message.append(\"Your rippening prosses will be stoped and your tomatoes will be susceptible to chilling injuries.\")\n",
        "                else:\n",
        "                    ripeness_message.append(\"Your tomatoes will ripen quickly, but you should bear in mind that they are susceptible to pathogens.\")\n",
        "            elif ripeness == \"unripe\":\n",
        "                if 8 <= temperature <= 12 and humidity > 50:\n",
        "                    ripeness_message.append(\"Unripe tomatoes detected. They are not edible but can be stored for up to 10 days untill they are edible.If you want to ripe them more slower , you should store them somewhere cooler and with less humidity\")\n",
        "                elif temperature < 8:\n",
        "                    ripeness_message.append(\"Your rippening prosses will be stoped and your tomatoes will be susceptible to chilling injuries.\")\n",
        "                else:\n",
        "                    ripeness_message.append(\"Your tomatoes will ripen quickly, but you should bear in mind that they are susceptible to pathogens.\")\n",
        "        return ripeness_message\n",
        "    else:\n",
        "        remaining_items = [item for item in pred_classes_names_level1 if item != \"Tomato\"]\n",
        "        return f\"No tomatoes detected. Items in the fridge: {', '.join(remaining_items)}\"\n",
        "\n",
        "# Streamlit UI\n",
        "def main():\n",
        "    # Κεντρικός τίτλος\n",
        "    st.markdown(\n",
        "        \"<h1 style='text-align: center;'>Welcome back</h1>\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "    # Εισαγωγή δεδομένων χρήστη\n",
        "    temperature = st.number_input(\n",
        "        \"Temperature (°C)\", value=8.0, min_value=-10.0, max_value=15.0, step=1.0\n",
        "    )\n",
        "    humidity = st.number_input(\n",
        "        \"Humidity (%)\", value=75.0, min_value=0.0, max_value=100.0, step=1.0\n",
        "    )\n",
        "\n",
        "    st.markdown(\n",
        "        \"<h3 style='text-align: center;'>What would you like to do?</h3>\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "    # Αρχικοποίηση Session State για την εικόνα\n",
        "    if \"selected_image\" not in st.session_state:\n",
        "        simulation_image_path = \"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/simulation\"\n",
        "        image_files = glob.glob(os.path.join(simulation_image_path, \"*.[jp][pn]g\"))\n",
        "        if not image_files:\n",
        "            st.error(\"No images found in the test directory.\")\n",
        "            st.stop()\n",
        "        st.session_state.selected_image = random.choice(image_files)\n",
        "\n",
        "    selected_image = st.session_state.selected_image\n",
        "\n",
        "    if st.button(\"See what's inside\"):\n",
        "        # Εκτέλεση Faster R-CNN για ανάλυση περιεχομένων ψυγείου\n",
        "        predictor1, metadata1 = setup_faster_rcnn_level1()\n",
        "        im = cv2.imread(selected_image)\n",
        "\n",
        "        # Εκτέλεση πρόβλεψης\n",
        "        outputs1 = predictor1(im)\n",
        "\n",
        "        # Οπτικοποίηση αποτελεσμάτων με Visualizer\n",
        "        v = Visualizer(\n",
        "            im[:, :, ::-1],  # Convert BGR to RGB\n",
        "            metadata=metadata1,\n",
        "            scale=0.8\n",
        "        )\n",
        "        out = v.draw_instance_predictions(outputs1[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        # Μετατροπή της εικόνας από OpenCV (numpy array) σε μορφή PNG\n",
        "        processed_img_path = \"fridge_analysis.png\"\n",
        "        cv2.imwrite(processed_img_path, out.get_image()[:, :, ::-1])  # Convert RGB to BGR for OpenCV\n",
        "\n",
        "        st.success(\"Analysis Complete!\")\n",
        "        # Εμφάνιση της εικόνας με ανιχνεύσεις\n",
        "        st.image(processed_img_path, caption=\"Detected Items in the Fridge\", use_container_width=True)\n",
        "\n",
        "        # Υπολογισμός αριθμού αντικειμένων\n",
        "        pred_classes_indices_level1 = outputs1[\"instances\"].pred_classes.cpu().numpy()\n",
        "        pred_classes_names_level1 = [metadata1.thing_classes[i] for i in pred_classes_indices_level1]\n",
        "        item_counts = {item: pred_classes_names_level1.count(item) for item in set(pred_classes_names_level1)}\n",
        "\n",
        "        st.write(\"Contents of the fridge:\")\n",
        "        for item, count in item_counts.items():\n",
        "            st.write(f\"- {count} {item}(s)\")\n",
        "\n",
        "    if st.button(\"See the maturity\"):\n",
        "        # Εκτέλεση Faster R-CNN για ανάλυση περιεχομένων ψυγείου\n",
        "        predictor1, metadata1 = setup_faster_rcnn_level1()\n",
        "        predictor2, metadata2 = setup_faster_rcnn_level2()\n",
        "        im = cv2.imread(selected_image)\n",
        "\n",
        "        # Εκτέλεση πρόβλεψης\n",
        "        outputs = predictor2(im)\n",
        "\n",
        "        # Οπτικοποίηση αποτελεσμάτων με Visualizer\n",
        "        v = Visualizer(\n",
        "            im[:, :, ::-1],  # Convert BGR to RGB\n",
        "            metadata=metadata2,\n",
        "            scale=0.8\n",
        "        )\n",
        "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "        # Μετατροπή της εικόνας από OpenCV (numpy array) σε μορφή PNG\n",
        "        processed_img_path = \"fridge_analysis.png\"\n",
        "        cv2.imwrite(processed_img_path, out.get_image()[:, :, ::-1])  # Convert RGB to BGR for OpenCV\n",
        "\n",
        "        st.success(\"Analysis Complete!\")\n",
        "        # Εμφάνιση της εικόνας με ανιχνεύσεις\n",
        "        st.image(processed_img_path, caption=\"Ripeness of tomatoes detected\", use_container_width=True)\n",
        "\n",
        "        # Ανάλυση ωριμότητας\n",
        "        ripeness_analysis = analyze_image(selected_image, temperature, humidity, predictor1, predictor2, metadata1, metadata2)\n",
        "\n",
        "        # Εμφάνιση ανάλυσης ωριμότητας\n",
        "        if isinstance(ripeness_analysis, list):\n",
        "            st.write(\"Ripeness Results:\")\n",
        "            for msg in ripeness_analysis:\n",
        "                st.write(\"- \" + msg)\n",
        "        else:\n",
        "            st.write(ripeness_analysis)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyVkUxTswiFA",
        "outputId": "d5ec4428-b497-4ffb-d0a8-e414d78e9f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting smart_fridge_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# Εκκίνηση Streamlit server\n",
        "os.system(\"streamlit run smart_fridge_app.py &\")\n",
        "\n",
        "# Σύνδεση με ngrok\n",
        "public_url = ngrok.connect(8501)  # Η θύρα πρέπει να είναι αριθμός χωρίς `port=`\n",
        "print(f\"Public URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8V2QUfr6QsO",
        "outputId": "3c5541eb-dead-449b-8e0c-302801e56f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://e0db-34-168-49-131.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Αξιολόγηση του Pipeline"
      ],
      "metadata": {
        "id": "MENDKmcx9HoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Δεδομένα σε coco format για την αξιολόγηση\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"F357b98Kl2koA8RqsGzW\")\n",
        "project = rf.workspace(\"simulation-sv1vx\").project(\"simulation-repenning\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"coco\")\n",
        "\n",
        "rf = Roboflow(api_key=\"F357b98Kl2koA8RqsGzW\")\n",
        "project = rf.workspace(\"simulation-sv1vx\").project(\"simulation-dsrze\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncl3N36A9Lfv",
        "outputId": "b4a34f48-00f3-4a5f-a532-7bfb2b6fca05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Simulation-repenning-2 to coco:: 100%|██████████| 584/584 [00:00<00:00, 2818.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Simulation-repenning-2 in coco:: 100%|██████████| 27/27 [00:00<00:00, 5282.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Simulation-1 to coco:: 100%|██████████| 585/585 [00:00<00:00, 2905.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Simulation-1 in coco:: 100%|██████████| 27/27 [00:00<00:00, 4833.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "import json\n",
        "\n",
        "\n",
        "# Καταχώριση των δεδομένων COCO και προσθήκη των κατηγοριών\n",
        "def register_coco_instances_with_classes(dataset_name, json_file, image_dir):\n",
        "    # Εγγραφή των δεδομένων COCO\n",
        "    register_coco_instances(dataset_name, {}, json_file, image_dir)\n",
        "\n",
        "    # Ανάγνωση του αρχείου json για να πάρουμε τις κατηγορίες\n",
        "    with open(json_file, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Λήψη των κατηγοριών από το αρχείο COCO\n",
        "    thing_classes = [category['name'] for category in coco_data['categories']]\n",
        "\n",
        "    # Ενημέρωση του metadata για να περιλαμβάνει τις κατηγορίες\n",
        "    metadata = MetadataCatalog.get(dataset_name)\n",
        "    metadata.thing_classes = thing_classes\n",
        "\n",
        "# Εγγραφή των δεδομένων για train, val, test και ενημέρωση των κατηγοριών\n",
        "register_coco_instances_with_classes(\"my_dataset_train_level1\", \"/content/level1_images-2/train/_annotations.coco.json\", \"/content/level1_images-2/train\")\n",
        "register_coco_instances_with_classes(\"my_dataset_test_level1\", \"/content/Simulation-1/train/_annotations.coco.json\", \"/content/Simulation-1/train\")\n",
        "\n",
        "register_coco_instances_with_classes(\"my_dataset_train_level2\", \"/content/Smart-Fridge-9/train/_annotations.coco.json\", \"/content/Smart-Fridge-9/train\")\n",
        "register_coco_instances_with_classes(\"my_dataset_test_level2\", \"/content/Simulation-repenning-2/valid/_annotations.coco.json\", \"/content/Simulation-repenning-2/valid\")\n",
        "\n",
        "# Ρύθμιση Faster R-CNN\n",
        "def setup_faster_rcnn_level1():\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(\"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/config1.yml\")\n",
        "    cfg.MODEL.WEIGHTS = \"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level1/model_final.pth\"\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75\n",
        "    predictor1 = DefaultPredictor(cfg)\n",
        "    metadata1 = MetadataCatalog.get(\"my_dataset_train_level1\")\n",
        "    return predictor1, metadata1,cfg\n",
        "\n",
        "def setup_faster_rcnn_level2():\n",
        "    cfg = get_cfg()\n",
        "    cfg.merge_from_file(\"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/config.yml\")\n",
        "    cfg.MODEL.WEIGHTS = \"/content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level2/model_final.pth\"\n",
        "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.75\n",
        "    predictor2 = DefaultPredictor(cfg)\n",
        "    metadata2 = MetadataCatalog.get(\"my_dataset_train_level2\")\n",
        "    return predictor2, metadata2,cfg\n",
        "\n",
        "predictor, metadata, cfg1 = setup_faster_rcnn_level1()\n",
        "predictor, metadata, cfg2 = setup_faster_rcnn_level2()\n",
        "os.makedirs(cfg1.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Φόρτωση Faster R-CNN και αξιολόγηση\n",
        "def evaluate_faster_rcnn(cfg, dataset_name):\n",
        "    evaluator = COCOEvaluator(dataset_name, output_dir=\"./output\")\n",
        "    val_loader = build_detection_test_loader(cfg, dataset_name)\n",
        "    results = inference_on_dataset(DefaultPredictor(cfg).model, val_loader, evaluator)\n",
        "    return results\n",
        "\n",
        "# Υπολογισμός συνολικής ακρίβειας\n",
        "# (προσαρμόζεται ανάλογα με τη χρήση των μοντέλων)\n",
        "def compute_overall_accuracy(faster_rcnn_metrics_1,faster_rcnn_metrics_2):\n",
        "    faster_rcnn_map1 = faster_rcnn_metrics_1[\"bbox\"]['AP']  # mAP του Faster R-CNN\n",
        "    faster_rcnn_map2 = faster_rcnn_metrics_2[\"bbox\"]['AP']  # mAP του Faster R-CNN\n",
        "    print(f\"Faster R-CNN mAP level1: {faster_rcnn_map1}\")\n",
        "    print(f\"Faster R-CNN mAP level2: {faster_rcnn_map2}\")\n",
        "    overall_accuracy = (faster_rcnn_map1 + faster_rcnn_map2) / 2  # Μέσος όρος\n",
        "    return overall_accuracy\n",
        "\n",
        "fast_rcnn_levle1=evaluate_faster_rcnn(cfg1, \"my_dataset_test_level1\")\n",
        "fast_rcnn_levle2=evaluate_faster_rcnn(cfg2, \"my_dataset_test_level2\")\n",
        "\n",
        "overall_accuracy = compute_overall_accuracy(fast_rcnn_levle1, fast_rcnn_levle2)\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xptRjwpj9RIh",
        "outputId": "6a72201a-4dc5-41e9-d3f8-7c1a840701fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/13 11:35:14 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level1/model_final.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/13 11:35:24 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level2/model_final.pth ...\n",
            "WARNING [02/13 11:35:33 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[02/13 11:35:33 d2.data.datasets.coco]: Loaded 23 images in COCO format from /content/Simulation-1/train/_annotations.coco.json\n",
            "[02/13 11:35:33 d2.data.build]: Distribution of instances among all 9 categories:\n",
            "|  category  | #instances   |  category   | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:-----------:|:-------------|:----------:|:-------------|\n",
            "|  objects   | 0            |    Apple    | 15           |   Banana   | 10           |\n",
            "|    Lime    | 7            |   Orange    | 5            |   Peach    | 6            |\n",
            "|    Pear    | 3            | Pomegranate | 9            |   Tomato   | 49           |\n",
            "|            |              |             |              |            |              |\n",
            "|   total    | 104          |             |              |            |              |\n",
            "[02/13 11:35:33 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[02/13 11:35:33 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[02/13 11:35:33 d2.data.common]: Serializing 23 elements to byte tensors and concatenating them all ...\n",
            "[02/13 11:35:33 d2.data.common]: Serialized dataset takes 0.01 MiB\n",
            "[02/13 11:35:34 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level1/model_final.pth ...\n",
            "[02/13 11:35:35 d2.evaluation.evaluator]: Start inference on 23 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/13 11:35:39 d2.evaluation.evaluator]: Inference done 11/23. Dataloading: 0.0013 s/iter. Inference: 0.1005 s/iter. Eval: 0.0003 s/iter. Total: 0.1021 s/iter. ETA=0:00:01\n",
            "[02/13 11:35:40 d2.evaluation.evaluator]: Total inference time: 0:00:01.876343 (0.104241 s / iter per device, on 1 devices)\n",
            "[02/13 11:35:40 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:01 (0.099944 s / iter per device, on 1 devices)\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[02/13 11:35:40 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[02/13 11:35:40 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
            "[02/13 11:35:40 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[02/13 11:35:40 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.02 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.837\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.973\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.903\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.837\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.796\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.872\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.872\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.872\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 83.701 | 97.294 | 90.322 |  nan  |  nan  | 83.701 |\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[02/13 11:35:40 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category    | AP     | category   | AP     |\n",
            "|:-----------|:-------|:------------|:-------|:-----------|:-------|\n",
            "| objects    | nan    | Apple       | 77.340 | Banana     | 84.158 |\n",
            "| Lime       | 84.076 | Orange      | 92.030 | Peach      | 90.842 |\n",
            "| Pear       | 67.690 | Pomegranate | 87.217 | Tomato     | 86.257 |\n",
            "WARNING [02/13 11:35:40 d2.data.datasets.coco]: \n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "[02/13 11:35:40 d2.data.datasets.coco]: Loaded 23 images in COCO format from /content/Simulation-repenning-2/valid/_annotations.coco.json\n",
            "[02/13 11:35:40 d2.data.build]: Distribution of instances among all 5 categories:\n",
            "|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
            "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
            "|   obgect   | 0            | half-ripe  | 16           | over-ripe  | 4            |\n",
            "|    ripe    | 10           |   unripe   | 19           |            |              |\n",
            "|   total    | 49           |            |              |            |              |\n",
            "[02/13 11:35:40 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[02/13 11:35:40 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[02/13 11:35:40 d2.data.common]: Serializing 23 elements to byte tensors and concatenating them all ...\n",
            "[02/13 11:35:40 d2.data.common]: Serialized dataset takes 0.01 MiB\n",
            "[02/13 11:35:41 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/gdrive/MyDrive/Georgios_Karagiannis/Code/D-level2/model_final.pth ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[02/13 11:35:43 d2.evaluation.evaluator]: Start inference on 23 batches\n",
            "[02/13 11:35:44 d2.evaluation.evaluator]: Inference done 11/23. Dataloading: 0.0084 s/iter. Inference: 0.1078 s/iter. Eval: 0.0005 s/iter. Total: 0.1168 s/iter. ETA=0:00:01\n",
            "[02/13 11:35:46 d2.evaluation.evaluator]: Total inference time: 0:00:02.119468 (0.117748 s / iter per device, on 1 devices)\n",
            "[02/13 11:35:46 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:01 (0.106144 s / iter per device, on 1 devices)\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Evaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "[02/13 11:35:46 d2.evaluation.fast_eval_api]: Evaluate annotation type *bbox*\n",
            "[02/13 11:35:46 d2.evaluation.fast_eval_api]: COCOeval_opt.evaluate() finished in 0.01 seconds.\n",
            "[02/13 11:35:46 d2.evaluation.fast_eval_api]: Accumulating evaluation results...\n",
            "[02/13 11:35:46 d2.evaluation.fast_eval_api]: COCOeval_opt.accumulate() finished in 0.02 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.709\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.835\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.835\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.709\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.736\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.757\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.757\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.757\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 70.939 | 83.511 | 83.511 |  nan  |  nan  | 70.939 |\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "[02/13 11:35:46 d2.evaluation.coco_evaluation]: Per-category bbox AP: \n",
            "| category   | AP     | category   | AP     | category   | AP     |\n",
            "|:-----------|:-------|:-----------|:-------|:-----------|:-------|\n",
            "| obgect     | nan    | half-ripe  | 70.266 | over-ripe  | 87.525 |\n",
            "| ripe       | 58.289 | unripe     | 67.677 |            |        |\n",
            "Faster R-CNN mAP level1: 83.70117011679278\n",
            "Faster R-CNN mAP level2: 70.93883091316651\n",
            "Overall Accuracy: 77.32000051497965\n"
          ]
        }
      ]
    }
  ]
}